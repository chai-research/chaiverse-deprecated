{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Llama7b Model with Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "import chai_guanaco as chai\n",
    "\n",
    "from chaiverse.dataset import DatasetLoader, CausalDatasetBuilder\n",
    "from chaiverse.tokenizer import LlamaTokenizer\n",
    "from chaiverse.trainer.causallm_trainer import CausalLMTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chai.developer_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process a small dataset\n",
    "\n",
    "We load a small subset of Chai's conversational dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = 'ChaiML/chaiverse_lora_testing_fandom_IO'\n",
    "data_loader = DatasetLoader(\n",
    "        hf_path=data_path,\n",
    "        data_samples=100,\n",
    "        validation_split_size=0.1,\n",
    "        shuffle=True,\n",
    "        )\n",
    "df = data_loader.load()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of the data. It includes `input_text` as prompt and `output_text` as response\n",
    "df['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "tokenizer = LlamaTokenizer()\n",
    "data_builder = CausalDatasetBuilder(\n",
    "        tokenizer_loader=tokenizer,\n",
    "        block_size=1024,\n",
    "        )\n",
    "data = data_builder.generate(df, n_jobs=10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "base_model = 'NousResearch/Llama-2-7b-hf'\n",
    "model = CausalLMTrainer(\n",
    "        model_name=base_model,\n",
    "        output_dir='test_lora',\n",
    "        use_lora=True,\n",
    "        )\n",
    "\n",
    "# can use model.update_training_config() to update TrainingArguments before trainer_setup\n",
    "# e.g. model.update_training_config(per_device_train_batch_size=16)\n",
    "\n",
    "model.trainer_setup(data)\n",
    "print(model.lora_config)\n",
    "print(model.training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fitting\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model, default to model.output_dir\n",
    "# can also specify save path : model.save(path=/alt/save/path)\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lora weights with base model weights\n",
    "# Defult to use the lora weights saved in mode.output_dir\n",
    "# can specify path to lora weights : model.merge(path=/alt/save/path)\n",
    "model.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to huggingface\n",
    "model_path = \"your/huggingface/model/path\"\n",
    "model.push_to_hub(model_path, private=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
